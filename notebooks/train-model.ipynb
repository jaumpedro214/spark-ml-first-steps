{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Machine Learning Model in Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession\n",
    "URL_SPARK = \"spark://spark:7077\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-ml\")\n",
    "    .config(\"executor.memory\", \"4g\")\n",
    "    .master(URL_SPARK)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "|_c0|               Date|AveragePrice|Total Volume|   4046|     4225| 4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|region|\n",
      "+---+-------------------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "|  0|2015-12-27 00:00:00|        1.33|    64236.62|1036.74| 54454.85|48.16|   8696.87|   8603.62|     93.25|        0.0|conventional|2015|Albany|\n",
      "|  1|2015-12-20 00:00:00|        1.35|    54876.98| 674.28| 44638.81|58.33|   9505.56|   9408.07|     97.49|        0.0|conventional|2015|Albany|\n",
      "|  2|2015-12-13 00:00:00|        0.93|   118220.22|  794.7|109149.67|130.5|   8145.35|   8042.21|    103.14|        0.0|conventional|2015|Albany|\n",
      "|  3|2015-12-06 00:00:00|        1.08|    78992.15| 1132.0| 71976.41|72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|\n",
      "+---+-------------------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_avocado = spark.read.csv(\"/data/avocado.csv\", header=True, inferSchema=True)\n",
    "# cache data\n",
    "df_avocado.cache()\n",
    "df_avocado.show(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import machine learning libraries\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Machine learning pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Import SQL transformer\n",
    "from pyspark.ml.feature import SQLTransformer, StandardScaler, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avocado_train, df_avocado_test = df_avocado.randomSplit([0.75, 0.25], seed=214)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting columns with SQLTransformer**\n",
    "\n",
    "This is a quite powerful transformer, which allows you to select and transform columns using SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+------------------+------------------+------------------+-----------------+----+-----+\n",
      "|AveragePrice|        type|          LOG 4225|          LOG 4770|    LOG Small Bags|    LOG Large Bags|  LOG XLarge Bags|year|month|\n",
      "+------------+------------+------------------+------------------+------------------+------------------+-----------------+----+-----+\n",
      "|        0.49|conventional|12.566747374652527| 9.362499927974252|11.166908098190957|10.313733879047971|              0.0|  15|   12|\n",
      "|        0.71|conventional|11.860764002611406| 9.647818872531012| 11.72123326879331| 10.40627082310141|9.322865162818028|  15|   12|\n",
      "|         0.8|conventional| 12.53017497505446|11.349393905288467|11.824526973139381| 9.415621332905047|9.658771095406955|  15|   12|\n",
      "|         0.8|conventional|13.028501871764691|11.364461534887267|13.490872079413348| 11.21667384527801|9.342104328605496|  15|   12|\n",
      "+------------+------------+------------------+------------------+------------------+------------------+-----------------+----+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = ['AveragePrice', 'type']\n",
    "COLUMNS = [f\"`{col}`\" for col in COLUMNS]\n",
    "\n",
    "LOG_COLUMNS =  ['4225', '4770', 'Small Bags', 'Large Bags', 'XLarge Bags']\n",
    "LOG_COLUMNS = [f\"LOG(`{col}`+1) AS `LOG {col}`\" for col in LOG_COLUMNS]\n",
    "\n",
    "sql_trans = SQLTransformer(\n",
    "    statement=f\"\"\"\n",
    "    \n",
    "    SELECT\n",
    "    {', '.join(COLUMNS)}\n",
    "    , {', '.join(LOG_COLUMNS)}\n",
    "    ,YEAR(__THIS__.Date)-2000 AS year\n",
    "    ,MONTH(__THIS__.Date) AS month\n",
    "\n",
    "    FROM __THIS__\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Visualize the data\n",
    "sql_trans.transform(df_avocado_train).show(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMaxScaler**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MinMaxScaler is a trasformer of type estimator. This kind of object needs to be fitted to the data before being used to transform it. So, the conlumns need to be converted to a vector first.\n",
    "\n",
    "The MinMaxScaler is a transformer that scales each feature individually to a given range (often [0, 1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------+\n",
      "|month|month_vec|month_scaled|\n",
      "+-----+---------+------------+\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "+-----+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "month_vec_ass = VectorAssembler(inputCols=['month'], outputCol='month_vec')\n",
    "month_vec_ass.transform(sql_trans.transform(df_avocado_train))\n",
    "\n",
    "df_avocado_month_ass = month_vec_ass.transform(sql_trans.transform(df_avocado_train))\n",
    "\n",
    "month_scaler = MinMaxScaler(inputCol='month_vec', outputCol='month_scaled')\n",
    "month_scaler = month_scaler.fit(df_avocado_month_ass)\n",
    "\n",
    "month_scaler.transform(df_avocado_month_ass).select( ['month', 'month_vec', 'month_scaled'] ).show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StringIndexer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigns a numerical value to each category in a column. As the column \"type\" only has two categories, it will be transformed into a column with only two values: 0 and 1, which is equivalent to applying a OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|        type|type_index|\n",
      "+------------+----------+\n",
      "|conventional|       0.0|\n",
      "|conventional|       0.0|\n",
      "|conventional|       0.0|\n",
      "|conventional|       0.0|\n",
      "+------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "str_indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n",
    "\n",
    "str_indexer = str_indexer.fit(df_avocado_train)\n",
    "\n",
    "str_indexer.transform(df_avocado_train).select( [\"type\", \"type_index\"] ).show(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VectorAssembler**\n",
    "\n",
    "This transformer combines a given list of columns into a single vector column. The vector column is named \"features\" by default, and it will be used later to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------------------------------------------------------------------------+------------+\n",
      "|features_cat|features_num                                                                                           |AveragePrice|\n",
      "+------------+-------------------------------------------------------------------------------------------------------+------------+\n",
      "|[0.0]       |[15.0,1.0,12.566747374652527,9.362499927974252,11.166908098190957,10.313733879047971,0.0]              |0.49        |\n",
      "|[0.0]       |[15.0,1.0,11.860764002611406,9.647818872531012,11.72123326879331,10.40627082310141,9.322865162818028]  |0.71        |\n",
      "|[0.0]       |[15.0,1.0,12.53017497505446,11.349393905288467,11.824526973139381,9.415621332905047,9.658771095406955] |0.8         |\n",
      "|[0.0]       |[15.0,1.0,13.028501871764691,11.364461534887267,13.490872079413348,11.21667384527801,9.342104328605496]|0.8         |\n",
      "+------------+-------------------------------------------------------------------------------------------------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply transformations\n",
    "## SQL transformer\n",
    "df_avocado_train_transformed = sql_trans.transform(df_avocado_train)\n",
    "\n",
    "## String indexer\n",
    "df_avocado_train_transformed = str_indexer.transform(df_avocado_train_transformed)\n",
    "\n",
    "## Month scaler (vector assembler + minmax scaler)\n",
    "df_avocado_train_transformed = month_vec_ass.transform(df_avocado_train_transformed)\n",
    "df_avocado_train_transformed = month_scaler.transform(df_avocado_train_transformed)\n",
    "\n",
    "# Join all features into a single vector\n",
    "numerical_vec_ass = VectorAssembler(\n",
    "    inputCols=['year', 'month_scaled', 'LOG 4225', 'LOG 4770', 'LOG Small Bags', 'LOG Large Bags', 'LOG XLarge Bags'],\n",
    "    outputCol='features_num'\n",
    ")\n",
    "df_avocado_train_transformed = numerical_vec_ass.transform(df_avocado_train_transformed)\n",
    "\n",
    "# Join all categorical features into a single vector\n",
    "categorical_vec_ass = VectorAssembler(\n",
    "    inputCols=['type_index'],\n",
    "    outputCol='features_cat'\n",
    ")\n",
    "df_avocado_train_transformed = categorical_vec_ass.transform(df_avocado_train_transformed)\n",
    "\n",
    "\n",
    "# See the result\n",
    "df_avocado_train_transformed.select(['features_cat', 'features_num', 'AveragePrice']).show(4, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StandardScaler**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a transformer that standardizes features by removing the mean and scaling to unit variance. It is very similar to the MinMaxScaler, but it does not bound the values to a specific range. It is also a type of estimator, so it needs to be fitted to the data before being used to transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features_scaled                                                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9527463109714546,1.0269649008115518,0.5657377199959452,0.8334134211814762,-0.6436162273445295]|\n",
      "|[-1.2177154955881637,1.6482225355667333,0.7058305701685025,1.0954357394643428,0.7803295242390127,0.8574417380503548,2.012648481596976]  |\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9399552148956506,1.5037797059140563,0.8203168521795554,0.6002078289352569,2.1083545825302594] |\n",
      "|[-1.2177154955881637,1.6482225355667333,1.1142436751287843,1.5073956355774096,1.4653967110976907,1.0678725104034048,2.0181300922626053] |\n",
      "|[-1.2177154955881637,1.6482225355667333,1.4787880607250015,1.8713767178927097,1.4321533934378963,1.1582533794554424,2.5870627060190463] |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std_scaler = StandardScaler(\n",
    "    inputCol=\"features_num\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "std_scaler = std_scaler.fit(df_avocado_train_transformed)\n",
    "std_scaler.transform(df_avocado_train_transformed).select(['features_scaled']).show(5, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join everything together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "|features                                                                                                                                    |AveragePrice|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9527463109714546,1.0269649008115518,0.5657377199959452,0.8334134211814762,-0.6436162273445295,0.0]|0.49        |\n",
      "|[-1.2177154955881637,1.6482225355667333,0.7058305701685025,1.0954357394643428,0.7803295242390127,0.8574417380503548,2.012648481596976,0.0]  |0.71        |\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9399552148956506,1.5037797059140563,0.8203168521795554,0.6002078289352569,2.1083545825302594,0.0] |0.8         |\n",
      "|[-1.2177154955881637,1.6482225355667333,1.1142436751287843,1.5073956355774096,1.4653967110976907,1.0678725104034048,2.0181300922626053,0.0] |0.8         |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "prepro_pipe = Pipeline(stages=[\n",
    "    sql_trans,\n",
    "    str_indexer,\n",
    "    month_vec_ass,\n",
    "    month_scaler,\n",
    "    numerical_vec_ass,\n",
    "    categorical_vec_ass,\n",
    "    std_scaler,\n",
    "\n",
    "    # Join all features into a single vector\n",
    "    VectorAssembler(\n",
    "        inputCols=['features_scaled', 'features_cat'],\n",
    "        outputCol='features'\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline_model = prepro_pipe.fit(df_avocado_train)\n",
    "\n",
    "# Transform the data\n",
    "df_avocado_train_transformed = pipeline_model.transform(df_avocado_train)\n",
    "\n",
    "# See the result\n",
    "df_avocado_train_transformed.select(['features', 'AveragePrice']).show(4, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Linear Regression Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a linear regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.8)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: AveragePrice)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 1000)\n",
      "predictionCol: prediction column name. (default: prediction, current: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "# Create a linear regression model\n",
    "lin_reg = LinearRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='AveragePrice',\n",
    "    predictionCol='prediction',\n",
    "    maxIter=1000,\n",
    "    regParam=0.3,       # Regularization\n",
    "    elasticNetParam=0.8 # Regularization mixing parameter. 1 for L1, 0 for L2.\n",
    ")\n",
    "\n",
    "# Explain parameter \n",
    "print(lin_reg.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|features                                                                                                                                    |AveragePrice|prediction        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9527463109714546,1.0269649008115518,0.5657377199959452,0.8334134211814762,-0.6436162273445295,0.0]|0.49        |1.4003505112793717|\n",
      "|[-1.2177154955881637,1.6482225355667333,0.7058305701685025,1.0954357394643428,0.7803295242390127,0.8574417380503548,2.012648481596976,0.0]  |0.71        |1.4003505112793717|\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9399552148956506,1.5037797059140563,0.8203168521795554,0.6002078289352569,2.1083545825302594,0.0] |0.8         |1.4003505112793717|\n",
      "|[-1.2177154955881637,1.6482225355667333,1.1142436751287843,1.5073956355774096,1.4653967110976907,1.0678725104034048,2.0181300922626053,0.0] |0.8         |1.4003505112793717|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "lin_reg_model = lin_reg.fit(df_avocado_train_transformed)\n",
    "\n",
    "# See the output\n",
    "df_avocado_train_pred = lin_reg_model.transform(df_avocado_train_transformed)\n",
    "df_avocado_train_pred.select(['features', 'AveragePrice', 'prediction']).show(4, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the model**\n",
    "\n",
    "The evaluator is a transformer that takes a dataset and returns a single value. In this case, it will return the RMSE value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3978489578943717"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_eval = RegressionEvaluator(\n",
    "    labelCol='AveragePrice',\n",
    "    predictionCol='prediction',\n",
    "    metricName='rmse' # Root mean squared error\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "reg_eval.evaluate(df_avocado_train_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Full Pipeline - Hyperparameter Tuning with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline = Pipeline(stages=[\n",
    "    prepro_pipe, # Preprocessing pipeline\n",
    "    lin_reg     # Linear regression model\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameter grid. To build a parameter grid, we use the ParamGridBuilder class. We use the original regression object to reference the parameters we want to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lin_reg.regParam, [0.0, 0.1, 0.3, 0.5]) \\\n",
    "    .addGrid(lin_reg.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimize metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_eval = RegressionEvaluator(\n",
    "    labelCol='AveragePrice',\n",
    "    predictionCol='prediction',\n",
    "    metricName='rmse' # Root mean squared error\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join everything together using a CrossValidator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_ml = CrossValidator(\n",
    "    estimator=ml_pipeline, \n",
    "    estimatorParamMaps=param_grid, \n",
    "    evaluator=reg_eval, \n",
    "    numFolds=4\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_ml_model = crossval_ml.fit(df_avocado_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model:  PipelineModel_dc90de555ac1\n",
      "Best score:  0.2833541578138277\n"
     ]
    }
   ],
   "source": [
    "best_model = crossval_ml_model.bestModel\n",
    "best_score = crossval_ml_model.avgMetrics[0]\n",
    "\n",
    "print(\"Best model: \", best_model)\n",
    "print(\"Best score: \", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score (RMSE): 0.2833541578138277\n",
      "\n",
      "LinearRegression_eeaa1d8bf6ea__aggregationDepth   , 2\n",
      "LinearRegression_eeaa1d8bf6ea__elasticNetParam    , 0.0\n",
      "LinearRegression_eeaa1d8bf6ea__epsilon            , 1.35\n",
      "LinearRegression_eeaa1d8bf6ea__featuresCol        , features\n",
      "LinearRegression_eeaa1d8bf6ea__fitIntercept       , True\n",
      "LinearRegression_eeaa1d8bf6ea__labelCol           , AveragePrice\n",
      "LinearRegression_eeaa1d8bf6ea__loss               , squaredError\n",
      "LinearRegression_eeaa1d8bf6ea__maxBlockSizeInMB   , 0.0\n",
      "LinearRegression_eeaa1d8bf6ea__maxIter            , 1000\n",
      "LinearRegression_eeaa1d8bf6ea__predictionCol      , prediction\n",
      "LinearRegression_eeaa1d8bf6ea__regParam           , 0.0\n",
      "LinearRegression_eeaa1d8bf6ea__solver             , auto\n",
      "LinearRegression_eeaa1d8bf6ea__standardization    , True\n",
      "LinearRegression_eeaa1d8bf6ea__tol                , 1e-06\n"
     ]
    }
   ],
   "source": [
    "best_lin_reg_params = best_model.stages[-1].extractParamMap()\n",
    "\n",
    "print(\"Best score (RMSE):\", best_score, end=\"\\n\\n\")\n",
    "for parameter, value in best_lin_reg_params.items():\n",
    "    print(f\"{str(parameter):50s}, {value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a baseline model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40128919777533023\n",
      "0.5046357636762134\n"
     ]
    }
   ],
   "source": [
    "# Creating a dummy regression model\n",
    "\n",
    "# Mean price\n",
    "mean_price = df_avocado_train.agg(F.mean('AveragePrice')).collect()[0][0]\n",
    "median_price = df_avocado_train.approxQuantile('AveragePrice', [0.5], 0.25)[0]\n",
    "\n",
    "mean_dummy_df = df_avocado_train.select('AveragePrice').withColumn('prediction', F.lit(mean_price))\n",
    "median_dummy_df = df_avocado_train.select('AveragePrice').withColumn('prediction', F.lit(median_price))\n",
    "\n",
    "# Evaluate the dummy models\n",
    "print(reg_eval.evaluate(mean_dummy_df))\n",
    "print(reg_eval.evaluate(median_dummy_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the best model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28368085199676235\n"
     ]
    }
   ],
   "source": [
    "df_avocado_test_pred = best_model.transform(df_avocado_test)\n",
    "\n",
    "# show scores\n",
    "print(reg_eval.evaluate(df_avocado_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
