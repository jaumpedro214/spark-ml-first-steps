{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Machine Learning Model in Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession\n",
    "URL_SPARK = \"spark://spark:7077\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-ml\")\n",
    "    .master(URL_SPARK)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "|_c0|               Date|AveragePrice|Total Volume|   4046|     4225| 4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|region|\n",
      "+---+-------------------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "|  0|2015-12-27 00:00:00|        1.33|    64236.62|1036.74| 54454.85|48.16|   8696.87|   8603.62|     93.25|        0.0|conventional|2015|Albany|\n",
      "|  1|2015-12-20 00:00:00|        1.35|    54876.98| 674.28| 44638.81|58.33|   9505.56|   9408.07|     97.49|        0.0|conventional|2015|Albany|\n",
      "|  2|2015-12-13 00:00:00|        0.93|   118220.22|  794.7|109149.67|130.5|   8145.35|   8042.21|    103.14|        0.0|conventional|2015|Albany|\n",
      "|  3|2015-12-06 00:00:00|        1.08|    78992.15| 1132.0| 71976.41|72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|\n",
      "+---+-------------------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_avocado = spark.read.csv(\"/data/avocado.csv\", header=True, inferSchema=True)\n",
    "# cache data\n",
    "df_avocado.cache()\n",
    "df_avocado.show(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import machine learning libraries\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Machine learning pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Import SQL transformer\n",
    "from pyspark.ml.feature import SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avocado_train, df_avocado_test = df_avocado.randomSplit([0.75, 0.25], seed=214)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting columns with SQLTransformer**\n",
    "\n",
    "This is a quite powerful transformer, which allows you to select and transform columns using SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------+----------+----------+-----------+------------+----+-----+\n",
      "|AveragePrice|     4225|    4770|Small Bags|Large Bags|XLarge Bags|        type|year|month|\n",
      "+------------+---------+--------+----------+----------+-----------+------------+----+-----+\n",
      "|        0.49|286858.37|11642.46|  70749.02|  30142.78|        0.0|conventional|2015|   12|\n",
      "|        0.71|141599.36|15486.97| 123158.22|  33065.33|    11190.0|conventional|2015|   12|\n",
      "|         0.8|276556.76|84912.97| 136560.04|   12277.7|   15657.53|conventional|2015|   12|\n",
      "|         0.8|455203.42|86202.11| 722787.61|  74359.03|   11407.39|conventional|2015|   12|\n",
      "+------------+---------+--------+----------+----------+-----------+------------+----+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = ['AveragePrice', '4225', '4770', 'Small Bags', 'Large Bags', 'XLarge Bags', 'type']\n",
    "COLUMNS = [f\"`{col}`\" for col in COLUMNS]\n",
    "\n",
    "sqlTransformer = SQLTransformer(\n",
    "    statement=f\"\"\"\n",
    "    \n",
    "    SELECT\n",
    "    {', '.join(COLUMNS)}\n",
    "    ,YEAR(__THIS__.Date) AS year\n",
    "    ,MONTH(__THIS__.Date) AS month\n",
    "\n",
    "    FROM __THIS__\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Visualize the data\n",
    "sqlTransformer.transform(df_avocado_train).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
